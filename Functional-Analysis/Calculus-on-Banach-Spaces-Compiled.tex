\documentclass[../main-manifolds.tex]{subfiles}

\begin{document}

\fchapter{1: Multilinear maps}\newpage
\topheader{Introduction}
A \emph{Banach space} is a normed vector space that is Cauchy-complete under the usual metric induced by its norm. 

If $E$ and $F$ are Banach spaces over $\real$. We will denote the norms on $E$, and $F$ by single lines, so 
\[
    \vert x\vert = \norm{x}_E\qqtext{and} \vert y\vert = \norm{y}_F\quad\forall x\in E,\: y\in F
\]
$\mathcal{L}(E,F)$ will denote the space of linear maps between $E$ and $F$. In the category of Banach spaces, the space of morphisms are called \emph{toplinear morphisms} - or continuous linear maps; which we will denote by $L(E,F)$ for toplinear morphisms bewteen $E$ and $F$. \\

We use $\norm{\cdot}_{L(E,F)}$ or $\norm{\cdot}$ to denote the operator norm, depending on how much emphasis we wish to place on $L(E,F)$. Recall,
\begin{align*}
\norm{\varphi}_{L(E,F)} &= \inf\bigset{A\geq 0,\: \vert\varphi(x)\vert\leq A\vert x\vert\:\forall x\in E}\\
&= \sup\bigset{\vert\varphi(x)\vert,\: x\in E,\: \vert x\vert=1}
\end{align*}

By the open mapping theorem: any continuous surjective linear map is an open map. Hence invertible elements in $L(E,F)$ are naturally called \emph{toplinear isomorphisms}. If $\varphi\in L(E,F)$ such that $\varphi$ preserves the norm between the Banach Spaces, that is for every $x\in E$, $\vert x\vert = \vert \varphi(x)\vert$ then we call $\varphi$ an \emph{isometry}, or a \emph{Banach space isomorphism}. If $E_1$ and $E_2$ are Banach spaces, we will use the usual \emph{product norm} $(x_1, x_2)\mapsto \max(\vert x_1\vert,\vert x_2\vert)$. 
\topheader{Bilinear maps}
\begin{definition}[Bilinear map]
A map $\varphi: E_1\times E_2\to F$, where $F$ is also a Banach space, is said to be \emph{bilinear} if
\[
    \varphi(x,\cdot): E_2\to F\qqtext{and}\varphi(\cdot,y): E_1\to F
\]
are linear for every $x\in E_1$ and $y\in E_2$.     
\end{definition}


\begin{wts}[Continuity of a bilinear map]\label{prop:characterization-of-continuity-bilinear-map}
    Let $E_1$, $E_2$, $F$ be Banach spaces, a bilinear map $m: E_1\times E_2\to F$ is continuous if and only if there exists a $C\geq 0$, where
    \begin{equation}\label{eq:continuity-bilinear-map}
        \vert m(x,y)\vert\leq C\vert x\vert\vert y\vert    
    \end{equation}
\end{wts}
\begin{proof}
    Suppose such a $C$ exists, fix a convergent sequence $(x_n, y_n)\to (x,y)$ in $E_1\times E_2 = E$. Because the projection maps are continuous, this means $x_n\to x$ and $y_n\to y$. Using inspiration from the proof where $x_ny_n\to xy$, where

    \[
        x_n(y_n-y) + (x_n-x)y = x_ny_n- xy\quad x,y,x_n,y_n\in\real
    \]

    Using the inspiration, and replacing multiplication in $\real$ with the bilinear map $m$, we have:
    \begin{align*}
        m(x_n,\: y_n-y) + m(x_n-x,\: y) &= m(x_n,\:y_n) - m(x,\:y)\\
        \vert m(x_n,\: y_n) - m(x,\: y)\vert &\leq C[\vert x_n\vert\cdot\vert y_n - y\vert + \vert x_n - x\vert\cdot\vert y\vert]\to 0
    \end{align*}

    Conversely, if $m$ is continuous, then it is continuous at the origin $(0,0)=0$. There exists a $\delta$ where $\vert (x,y)\vert \leq \delta$ implies $\vert m(x,y)\vert \leq 1$. Now, if $x,y\neq 0$ are elements in $E$, we normalize so that $(x,y)$ has length $\delta$

    \[
        \vert(x\vert x\vert^{-1}\delta,y\vert y\vert^{-1}\delta)\vert = \delta\vert(x\vert x\vert^{-1},y\vert y\vert^{-1})\vert = \delta
    \]

    So that $\vert m(x\vert x\vert^{-1}\delta, y\vert y\vert^{-1}\delta)\vert \leq 1$, using bilinearity of $m$:
    \[
        \vert m(x,y)\vert\leq \delta^{-2}\vert x\vert\cdot\vert y\vert
    \]

    Setting $\delta^{-2} = C$ finishes the proof (notice if eithe $x$ or $y$ is $0$, then $m$ is trivially $0$ and the inequality holds).
\end{proof}


\begin{wts}[$L(E_1,E_2;F)$ is isomorphic to $L(E_1, L(E_2,F))$]\label{prop:bilinear-map-isomorphism-currying}
    For each bilinear map $\omega\in L(E_1,E_2;F)$, there exists a unique map $\varphi_\omega\in L(E_1, L(E_2,F))$ such that $\vert \omega\vert = \vert\varphi_\omega\vert$; such that for every $(x,y)\in E_1\times E_2$, $\omega(x,y) = \varphi(x)(y)$.
\end{wts}
\begin{proof}
    Let $\varphi_\omega: E_1\to L(E_2,F)$ be the unique map such that $\varphi_\omega(x)(y) = \omega(x,y)$. \Cref{prop:characterization-of-continuity-bilinear-map} shows that $\varphi_\omega(x)$ is a continuous linear map into $F$ at each $x$, and $\vert\varphi_\omega(x)\vert\leq\vert\omega\vert\vert x\vert$. This holds for an arbitrary $x$, and $\varphi_{\omega}(\cdot)$ is clearly linear, hence $\vert\varphi_\omega\vert\leq\vert\omega\vert$. Reversing the roles of $\omega$ and $\varphi$ shows proves the other estimate.\\

    The rule as outlined above is linear in $\omega$; and it is not hard to see $\varphi: L(E_1,E_2; F)\to L(E_1, L(E_2, F))$ is an injection. By the open mapping theorem, the proposition is proven if $\varphi$ is a surjection. Fix $\theta\in L(E_1, L(E_2,F))$, define a map $\omega: E_1\times E_2\to F$ such that $\omega(x,\cdot) = \theta(x)(\cdot)$. So that $\omega$ is linear in its second argument. To show $\omega$ is linear in its first: fix a linear combination $A = \sum ax$ in $E_1$, and $y\in E_2$. 
    \[
    \omega(A,y) =\theta(\sum ax)(y) = \sum a\theta(x)(y) = \sum a\omega(x,y)
    \]
    Continuity follows from \Cref{eq:continuity-bilinear-map}, and $\varphi_\omega = \theta$ as needed.
\end{proof}
\topheader{Notation}
We will use the following notation to simplify computations with multilinear maps. Let $E$ and $F$ be sets, and $v_1,\ldots, v_k\in E$. $f: E\to F$.
\begin{itemize}
    \item Listing individual elements: $\UL{v}[k]$ means $v_1,\ldots,v_k$ as separate elements. 
    \item Creating a $k$-list: $(\UL{v}[k]) = (v_1,\ldots, v_k)\in \prod E_{j\leq k}$ if $v_i\in E_i$ for $i = \underline{k}$.
    \item Double indices: $(\UL{v}[n_k]) = (v_{n_{\underline{k}}}) = (v_{n_1},\ldots, v_{n_k})$, and
    \[
        (\UL{v}[n_k]) \neq (v_{n_(1,\ldots,k)})
    \]
    \item Closest bracket convention:
    \[
        (v_{(n_{\underline{k}})}) = (v_{(n_1, \ldots, n_k)})\qqtext{and}(v_{n_{(\underline{k})})}) = (v_{n_{(1, \ldots, k)}})
    \]
    \item Underlining $0$ means it is iterated $0$ times: 
    \[
        (\UL{v}[0], a,b,c) = (a,b,c)
    \]
    \item Skipping an index: 
    \[
        (\UL{v}[i-1],\UL{v}[i+][k-i]) = (v_{1},\ldots, v_{i-1},v_{i+1},\ldots, v_{k})
    \]
    for $i = \underline{k}$.
    \item Applying $f$ to a particular index: 
    \[
        (\UL{v}[i-1], f(v_{i}), \UL{v}[i+][k-i]) = (v_1,\ldots, v_{i-1}, f(v_i), v_{i+1}, \ldots, v_k)
    \]
    Of course, if $i=1$, then the above expression reads $(f(v_1), v_2\ldots, v_k)$ by the $\underline{0}$ interpretation.
    \item In any list using this 'underline' notation, we can find the size of a list by summing over all the underlined terms, and the number of terms with no underline.
    \item If $\wedge: E\times E\to F$ is any associative binary operation,
    \[
        \bigowedge(\UL{v}[k]) = v_1\wedge\cdots \wedge v_k
    \]
\end{itemize}
\begin{remark}[Preview of exterior calculus]
We can write the formula for the determinant of a $\real^{k\times k}$ matrix in this notation. Suppose $a_i\in\real$, and $b_i\in\real^{k-1}$ for $i=\underline{k}$.

\[
M = \begin{bmatrix}
    a_1 & \cdots & a_k \\[1ex]
    \vert &  & \vert \\
    b_1 & \cdots & b_k \\
    \vert &  & \vert \\[1ex]
\end{bmatrix}
\]
The determinant of $M$ is a linear combination of determinants of $k-1$-sized matrices, given in terms of the columns of $b$
\[
    \det(M) = \sum_{i=\underline{k}}(-1)^{i-1} a_i\det(\UL{b}[i-1],\UL{b}[i+][k-i])
\]    
\end{remark}

\topheader{$k$-linear maps}
\begin{definition}[$k$-linear maps]\label{def:k-linear-maps}
    Let $\UL{E}[k]$, $F$ be Banach spaces. A map $\varphi: \prod \UL{E}[k]$ is $k$-linear if for every $i=\underline{k}$, $v_i\in E_i$, 
    \[
        \varphi(\UPL{\cdot}[i-1],v_i,\UPL{\cdot}[k-i]):\: \bigoprod (\UL{E}[i-1],\UL{E}[i+][k-i])\to F\quad\text{is }(k-1)\text{-linear}
    \]
\end{definition}
The following theorem should give confidence to the notation we have adopted to use.
\begin{wts}
    Let $\UL{E}[k]$ and $F$ be Banach spaces, a $k$-linear map $\varphi: \prod \UL{E}[k]\to F$ is continuous iff there exists a $C>0$, such that for every $x_i\in E_i$, $i=\underline{k}$
    \[
        \abs{\varphi(\UL{x}[k])} \leq C\prod \abs{\UL{x}[k]}
    \]
\end{wts}
\begin{proof}
    Suppose $\varphi$ is continuous, then it is continuous at the origin. Picking $\varepsilon = 1$ induces a $\delta>0$ such that for $\abs{(\UL{x}[k])}\leq \delta$, $\abs{\varphi(\UL{x}[k])}\leq 1$. The usual trick of normalizing an arbitrary vector $(\UL{x}[k])\in \prod \UL{E}[k]$ does the job:
    \[
        \abs{\varphi(x_k\cdot \abs{\UL{x}[k]}^{-1}\cdot\delta)}\leq 1\implies \abs{\varphi(\UL{x}[k])}\leq \delta^{-k}\prod\abs{\UL{x}[k]}
    \]
    Conversely, fix a sequence (indexed by $n$, in $k$ elements in the product space $\prod\UL{E}[k]$), so
    \begin{equation}\label{eq:k-linear-sequence-def}
        (x_n^{\underline{k}})\to (x^{\underline{k}})\quad \text{as } n\to +\infty
    \end{equation}
    To proceed any further, we need to prove an important equation that decomposes a difference in $\varphi$.
    \begin{equation}\label{eq:k-linear-lemma-1}
        \varphi(\UPL{b}[k]) - \varphi(\UPL{a}[k]) = \sum_{i=\underline{k}}\varphi(\UPL{b}[i-1],\Delta_i, \UPL{a}[i+][k-i])
    \end{equation}
    where $(\UPL{b}[k])$ and $(\UPL{a}[k])$ are elements in $\prod \UL{E}[k]$, and $\Delta_i = b^i - a^i$ for $i = \underline{k}$. The proof is in the following note, which is in more detail than usual - to help the reader ease into the new notation.
    \begin{note}
        We proceed by induction, and \cref{eq:k-linear-lemma-1} follows by setting $m=k$ in
        \begin{equation}\label{eq:k-linear-lemma-2}
            \varphi(\UPL{a}[k]) = \varphi(\UPL{b}[m],\UPL{a}[m+][k-m]) - \sum_{i=\underline{m}}\varphi(\UPL{b}[i-1],\Delta_i,\UPL{a}[i+][k-i])
        \end{equation}
        Base case: set $m=1$, by definition of $k$-linearity (\cref{def:k-linear-maps}) of $\varphi$. Since $a^1 = b^1-\Delta_1$, 
        \[
            \varphi(\UPL{a}[k]) = \varphi(b^1-\Delta_1, \UPL{a}[1+][k-1]) = \varphi(b^1, \UPL{a}[1+][k-1]) - \varphi(\Delta_1,\UPL{a}[1+][k-1])
        \]
        Induction hypothesis: suppose \cref{eq:k-linear-lemma-2} holds for a fixed $m$. Since $a^{m+1} = b^{m+1} - \Delta_{m+1}$, 
        \begin{align*}
            \varphi(\UPL{a}[k]) &= \varphi(\UPL{b}[m],\UPL{a}[m+][k-m]) - \sum_{i=\underline{m}}\varphi(\UPL{b}[i-1],\Delta_i,\UPL{a}[i+][k-i])\\
            &= \varphi(\UPL{b}[m],a^{m+1},\UPL{a}[(m+1)+][k-(m+1)]) - \sum_{i=\underline{m}}\varphi(\UPL{b}[i-1],\Delta_i,\UPL{a}[i+][k-i])\\
            &= \varphi(\UPL{b}[m+1],\UPL{a}[(m+1)+][k-(m+1)]) - \varphi(\UPL{b}[m+1],\Delta_{m+1},\UPL{a}[(m+1)+][k-(m+1)]) - \sum_{i=\underline{m}}\varphi(\UPL{b}[i-1],\Delta_i,\UPL{a}[i+][k-i])
        \end{align*}
        and this proves \cref{eq:k-linear-lemma-1}
    \end{note}
    We substitute $a^i = x^i$, and $b^i = x_n^i$ for $i = \underline{k}$, and \cref{eq:k-linear-lemma-1} becomes \cref{klinear-eq}
    \begin{equation}\label{klinear-eq}
        \varphi(x_n^{\underline{k}}) - \varphi(x^{\underline{k}}) = \sum_{i=\underline{k}}\varphi(x_n^{\underline{i-1}}, x_n^i - x^i, x^{i+\underline{k-i}})
    \end{equation}
    Then the triangle inequality reads
    \begin{align*}
        \abs{\varphi(x_n^{\underline{k}}) - \varphi(x^{\underline{k}})} &\leq \sum_{i=\underline{k}} \abs{\varphi(x_n^{\underline{i-1}}, x_n^i - x^i, x^{i+\underline{k-i}})}\\
        &\leq \sum_{i=\underline{k}}\abs{\varphi}\cdot\bigoprod\qty({\UPL{x_n}[i-1],\Delta_i,\UPL{x}[i+][k-i]})\\
        &\leq \sum_{i=\underline{k}}\abs{\varphi}\cdot\abs{x_n^i - x^i}\bigoprod\qty(x_n^{\underline{i-1}}, x^{i+\underline{k-i}})\\
        &\Lsim_n\vert\varphi\vert\sup_{i=\underline{k}}\vert x_n^i - x^i\vert\to 0
    \end{align*}
    where we identify the product $\bigoprod\qty(\UPL{v}[k])$ with the product of their norms $\bigoprod \qty(\abs{\UPL{v}[k]})$. 
\end{proof}
\begin{remark}
    The $k$-linear variant of \cref{prop:bilinear-map-isomorphism-currying} holds. We will use but not prove this fact.
\end{remark}
\begin{remark}
We denote the space of $k$-linear maps from $E$ into $F$ by $L(\UL{E}[k]; F) = L(E^k, F) = L^k(E,F)$. \emph{Tensors} on $E$ are $k$-linear maps from the product space of $E$ into $\real$, by replacing $F$ with $\real$.
\end{remark}
%
% What to include also?
% Should we start from defining the properties of Banach Spaces? TVS?
%

\fchapter{2: Differentiation}\newpage
\topheader{The derivative}
\begin{definition}[Open sets and neighbourhoods]\label{def:osub-notation}
    If $U$ is an open subset of a topological space $X$, we denote this by $U\osub X$. If $U$ is a \emph{neighbourhood} of a point $p\in X$, we write $p\oin U$. \\
    
    We do not require neighbourhoods to be open sets; rather, we say $U$ is a neighbourhood of $p$ when the interior of $U$ contains $p$.
\end{definition}
\begin{definition}[Little $o$]\label{def:little-oh}
    A real-valued function in a real variable defined for all $t$ sufficiently small is said to be \emph{$o(t)$} if $\lim_{t\to 0}o(t)/t=0$. A map $\psi: U\to  F$ where $U\osub E$ contains $0$ in $E$, is said to be $o(h)$ if $\vert \psi(h)\vert/\vert h\vert \to 0$ as $h\to 0$ in $E$.
\end{definition}
\begin{definition}[Differentiability]\label{def:differentiability}
    Let $f: E\to F$ be a map, replacing $E$ and $F$ by their open subsets if necessary. We say $f$ is \emph{differentiable} at $x\in E$ when there exists a \textbf{continuous linear map on $E$}: $\lambda\in L(E,F)$ such that
    \begin{equation}\label{eq:differentiability}
        f(x+h) = f(x) + \lambda h + o(h)\quad\text{for sufficiently small }h
    \end{equation}
    The role $o(h)$ plays here is a map from $U\to F$, where $U$ is some neighbourhood of $0$. 
\end{definition}
\begin{wts}[Basic properties of the derivative]\label{prop:basic-properties-of-derivative}
    If $f$ is differentiable at $x$, then the $\lambda$ in \cref{eq:differentiability} is unique. We write $f'(x) = Df(x) = \lambda$ as in \cref{eq:differentiability-def}. Furthermore, if $f'(x)$ and $g'(x)$ exist, then $(f+g)'(x) = f'(x) + g'(x)$ as linear maps, similar for scalar multiplication.
\end{wts}
\begin{proof}
    Suppose $\lambda_i\in L(E,F)$ are both derivatives of $f$ at $x$. Then,
    \[
        \begin{cases}
        f(x+h) = f(x) + \lambda_1(h) + o(h)\\
        f(x+h) = f(x) + \lambda_2(h) + o(h)
        \end{cases}
    \]
    And $(\lambda_1 - \lambda_2)(h) = o(h) = \varphi(h)\cdot\abs{h}$, where $\varphi(h)\to 0$ as $h\to 0$. Using the operator norm, we see that
    \[
        \norm{\lambda_1-\lambda_2}_{L(E,F)}\leq \abs{\varphi(h)}\to 0
    \]
    This proves uniqueness. Suppose $f$ and $g$ are differentiable at $x$, denote $\lambda_f = f'(x)$ (resp. $g'(x)$). The definition of \cref{def:differentiability} reads
    \begin{align}
        f(x+h) + g(x+h) &= \qty(f(x) + g(x)) + \qty(\lambda_f(h) + \lambda_g(h)) + o(h) + o(h)\nonumber\\
        (f + g)(x+h) &= (f+g)(x) + (\lambda_f + \lambda_g)(h) + o(h)\label{eq:differentiable-addition-eq}
    \end{align}
    since \cref{eq:differentiable-addition-eq} satisfies \cref{eq:differentiability}, the proof is complete.
\end{proof}
\begin{wts}[Chain rule]\label{prop:chain-rule}
    Let $E, F, G$ be Banach spaces. If $f\in C^1(E,F)$, $g\in C^1(F,G)$, for every $x\in E$,
    \begin{equation}\label{eq:chain-rule}
        (g\circ f)'(x) = g'(f(x))\circ f'(x)
    \end{equation}
\end{wts}
\begin{proof}
    Since $f$ is differentiable at $x$, $f(x+h) = f(x) + f'(x)(h) + o_1(h)$, (resp. for $g$, $o_2(h)$). Set $k(h) = f(x+h) - f(x)$, and 
    \begin{align}
        g(f(x+h)) &= g(f(x)) + g'(f(x))(k(h)) + o_2(k(h))\\
        &= g(f(x)) + g'(f(x))(f'(x)(h) + o_1(h)) + o_2(k(h))\\
        (g\circ f)(x+h) &= (g\circ f)(x) + g'(f(x))\circ f'(x)(h) + g'(f(x))(o_1(h)) + o_2(k(h))
    \end{align}
        
    
\end{proof}
\begin{wts}[Product rule in $k$ variables]\label{prop:product-rule-k-variables}
    Let $m: \prod \UL{F}[k]\to G$ be a $k$-linear map between Banach spaces $\UL{F}[k]$ and $G$. Suppose $f_i\in C^1(E, F_i)$ with $i=\underline{k}$, writing 
    \begin{equation}\label{eq:k-linear-multiplication-map-def}
        m(\UL{f}[k])(x) = m(\UL{f}[k](x))
    \end{equation}
    then $m(\UL{f}[k])$ is in $C^1(E,G)$ and for every $y\in E$,
    \begin{equation}\label{eq:k-linear-product-rule}
        Dm(\UL{f}[k])(x)(y) = \sum_{i=\underline{k}}m(\UL{f}[i-1](x),Df_i(x)(y),\UL{f}[i+][k-i](x))
    \end{equation}
\end{wts}
\begin{proof}
    Let $x$ be fixed. \Cref{eq:k-linear-product-rule} is proven if we show \cref{eq:product-rule-k-variables-modified-equation}
    \begin{equation}\label{eq:product-rule-k-variables-modified-equation}
        m(\UL{f}[k])(x+h) = m(\UL{f}[k])(x) + \qty(\sum_{i=\underline{k}}m(\UL{f}[i-1](x), Df_i(x)(h),\UL{f}[i+][k-i](x))) + o(h)
    \end{equation}
    and for sufficiently small $h$ we have
    \begin{equation}\label{eq:product-rule-k-variables-o(h)}
        f_i(x+h) - f_i(x) = Df_i(x)(h) + o(h^i)
    \end{equation}
    We will use the difference formula in \cref{eq:k-linear-lemma-2}, with the following substitutions
    \begin{align}
        f_i(x+h) &= b^i & f_i(x) &=a^i\label{eq:product-rule-k-variables-sub-1}\\
        Df_i(x)(h) &= c^i &  o(h^i)&=\varepsilon^i\label{eq:product-rule-k-variables-sub-2}\\
        f_i(x+h) - f_i(x) &= c^i + \varepsilon^i & \Delta^i &= o(h^i) + c^i\label{eq:product-rule-k-variables-sub-3}
    \end{align}
    With these substitutions, the equation we want to prove (\cref{eq:k-linear-product-rule}) becomes \cref{eq:product-rule-k-variables-modified-k-linear}
    \begin{equation}\label{eq:product-rule-k-variables-modified-k-linear}
        m(\UPL{b}[k]) - m(\UPL{a}[k]) = \qty(\sum_{i=\underline{k}}m(\UPL{a}[i-1],c^i,\UPL{a}[i+][k-i])) + o(h)
    \end{equation}
    Starting from \cref{eq:k-linear-lemma-2}, 
    \[
    m(\UPL{b}[k]) - m(\UPL{a}[k]) = \sum_{i=\underline{k}}m(\UPL{b}[i-1],\Delta^i,\UPL{a}[i+][k-i])
    \]
    We can expand each term, if $i=\underline{k}$, 
    \begin{equation}\label{eq:product-rule-k-variables-expand-1}
        m(\UPL{b}[i-1],\Delta^i,\UPL{a}[i+][k-i]) = m(\UPL{b}[i-1],c^i,\UPL{a}[i+][k-i])+ m(\UPL{b}[i-1],o(h^i),\UPL{a}[i+][k-i])
    \end{equation}
    Let us study the first term in \cref{eq:product-rule-k-variables-expand-1}, and with $i$ held fixed, define
    \begin{equation}\label{eq:product-rule-k-variables-m_i-function}
        m_i(\UPL{z}[i-1]) = m(\UPL{z}[i-1],c_i,\UPL{a}[i+][k-i])
    \end{equation}
    Expanding the first term within \cref{eq:product-rule-k-variables-expand-1}, and because $m_i$ as defined in \cref{eq:product-rule-k-variables-m_i-function} is $i-1$-linear (because it is a $k$-linear map with $k-(i-1)$ variables held constant); we use \cref{eq:k-linear-lemma-2} again.
    \begin{equation}\label{eq:product-rule-k-variables-expand-2}
        m_i(\UPL{b}[i-1]) = \qty(\sum_{i=\underline{k}} m_i(\UPL{b}[j], \Delta^j, \UPL{a}[j+][(i-1)-j])) + m_i(\UPL{a}[i-1])
    \end{equation}
    Unboxing the last term in \cref{eq:product-rule-k-variables-expand-2} using the definition of $m_i$ reads
    \begin{equation}\label{eq:product-rule-k-variables-expand-3}
        m(\UPL{b}[i-1],\Delta^i,\UPL{a}[i+][k-i]) = m(\UPL{a}[i-1],c^i,\UPL{a}[i+][k-i]) + \sum_{j=\underline{i-1}}m_i(\UPL{b}[j],\Delta^j,\UPL{a}[j+][(i-1)-j])
    \end{equation}
    We wish to remove all of the $b^i$s. Since $\Delta^i = c^i + \varepsilon^i$ (\cref{eq:product-rule-k-variables-sub-3}), we have
    \begin{align}
        m(\UPL{b}[k]) - m(\UPL{a}[k]) &= \sum_{i=\underline{k}} m(\UPL{b}[i-1],c^i,\UPL{a}[i+][k-i]) + m(\UPL{b}[i-1],\varepsilon^i,\UPL{a}[i+][k-i])\nonumber\\
        &= \qty(\sum_{i=\underline{k}}m_i(\UPL{b}[i-1])) + \sum_{i=\underline{k}}m(\UPL{b}[i-1],\varepsilon^i,\UPL{a}[i+][k-i])\nonumber\\
        &= \qty(\sum_{i=\underline{k}} m_i(\UPL{a}[i-1]) + \sum_{j=\underline{i-1}}m_i(\UPL{b}[j-1],\Delta^{j},\UPL{a}[j+][(i-1)-j])) + \sum_{i=\underline{k}} m(\UPL{b}[i-1],\varepsilon^i,\UPL{a}[i+][k-i])\nonumber\\
        &= \qty(\sum_{i=\underline{k}}m_i(\UPL{a}[i-1]))  + \sum_{\substack{i=\underline{k}\\ j=\underline{i-1}}} m_i(\UPL{b}[j-1],\Delta^j,\UPL{a}[j+][(i-1)-j]) + \sum_{i=\underline{k}}m(\UPL{b}[i-1], \varepsilon^i,\UPL{a}[i+][k-i])\label{eq:product-rule-k-variables-expand-4}
    \end{align}
    The last term within \cref{eq:product-rule-k-variables-expand-4} is $o(h)$, since it is a linear combination of $o(h^i)$s. 
    \begin{equation}\label{eq:product-rule-k-variables-last-term-o(h)}
    \abs{\sum_{i=\underline{k}}m(\UPL{b}[i-1], \varepsilon^i,\UPL{a}[i+][k-i])}\Lsim_{m,a,b}\vert o(h)\vert
    \end{equation}
    Each summand in the second last term in \cref{eq:product-rule-k-variables-expand-4} is $o(h)$ as well, as
    \begin{align}
        \abs{m_i(\UPL{b}[j-1],\Delta^j,\UPL{a}[j+][(i-1)-j])}&\leq\vert m_i\vert\bigoprod(\UPL{b}[j-1],\Delta^j,\UPL{a}[j+][(i-1)-j])\nonumber\\
        &\leq \abs{m}\cdot\qty(\bigoprod(c^i,\UPL{a}[i+][k-i]))\qty(\bigoprod(\UPL{b}[j-1],\Delta^j,\UPL{a}[j+][(i-1)-j]))\nonumber\\
        &\Lsim_{m,a,b}\sup_{\substack{i=\underline{k}\\ j = \underline{i-1}}}\abs{c^i}\cdot\abs{\Delta^j}\nonumber\\
        &\Lsim_{m,a,b}\sup_{\substack{i=\underline{k}\\ j = \underline{i-1}}}\abs{Df_i(x)(h)}\cdot\abs{f_j(x+h) - f_j(x)}\nonumber\\
        &\Lsim_{m,a,b}\abs{Df_i(x)}\abs{h}\sup_{\substack{i=\underline{k}\\ j=\underline{i-1}}}\abs{\Delta^j}\nonumber\\
        &\Lsim_{m,a,b}\abs{o(h)}\label{eq:product-rule-k-variables-second-last-term-o(h)}
    \end{align}
    for the second last estimate we used $\Delta^j\to 0$. Therefore the second term in \cref{eq:product-rule-k-variables-expand-4} is $o(h)$, and \cref{eq:product-rule-k-variables-modified-equation} is proven. Therefore $m(\UL{f}[k])$ is differentiable at $x$. Continuity of $Dm(\UL{f}[k])$ follows from the fact that 
    \begin{equation}\label{eq:product-rule-k-variables-continuity-0}
        Dm(\UL{f}[k])(x) = \sum_{i=\underline{k}}m(\UL{f}[i-1](x),Df_i(x)(\cdot),\UL{f}[i+][k-i](x))
    \end{equation}
    and each of the summands \cref{eq:product-rule-k-variables-continuity-0} can be broken down as the product of the compositions shown in \cref{eq:product-rule-k-variables-continuity-1,eq:product-rule-k-variables-continuity-2}
    \begin{align}
        x\mapsto(\UL{f}[i-1](x),\UL{f}[i+][k-i](x))\mapsto m(\UL{f}[i-1](x),\cdot,\UL{f}[i+][k-i](x))\label{eq:product-rule-k-variables-continuity-1}\\
        x\mapsto Df_i(x)(\cdot)\label{eq:product-rule-k-variables-continuity-2}
    \end{align}
    which are continuous from $E$ to $L(E,F)$.
\end{proof}


\fchapter{4: Higher order derivatives}\newpage
\topheader{Introduction}
We start with the definition of $C^p(E,F)$. Let $E$ and $F$ be Banach Spaces, if $p\geq 1$ is an integer, we define the class $C^p$ to be the set of maps which are $p$ times differentiable, and $D^p f\in C(E, X)$, where 

\[
    X = L(E, L(E, L(E,\cdots F))  p\text{ times }  \Isomor{L} L(E^p, F)
\]

Sometimes we replace $E$ with an open subset $U\subseteq E$ if necessary, and we write $f\in C(U,F)$ if $D^p\in C(U,X)$. Note, even if $f\in C^1(U,F)$, $Df$ is still a map from $U$ into $L(E,F)$. 

We will prove two major results in this section.

\begin{itemize}
    \item The structure of the derivative $D^p f$, in particular, if $f\in C^p(E,F)$, then $D^pf(x)$ is a \emph{symmetric multilinear map} in $p$ arguments. 
    \item Taylor's Theorem
\end{itemize}

\topheader{The second derivative}
\begin{wts}[Product rule in $2$ variables]
    Let $E_1$, $E_2$ and $F$ be Banach spaces, if $\omega: E_1\times E_2\to F$ is bilinear and continuous, then $\omega$ is differentiable, and for every $(x_1, x_2)\in E_1\times E_2$, $(v_1,v_2)\in E_1\times E_2$,

    \[
        D\omega(x_1,x_2)(v_1,v_2) = \omega(x_1, v_2) + \omega(v_1, x_2)
    \]

    Furthermore, $D^2\omega(x,y) = D\omega\in L(E^2,F)$, and $D^3\omega = 0$.
    
\end{wts}
\begin{proof}
    By the definition of $\omega$, using the familiar interpolation method
    \[
        \omega(x_1 + h_2, x_2 + h_2) = \omega(x_1, x_2)  + \omega(x_1, h_2) + \omega(h_1, x_2) + \omega(h_1, h_2)
    \]
    by continuity of $\omega$, the last term (which we wish to make $o(h)$): 
    \[
    \vert \omega(h_1, h_2)\vert\leq \norm{\omega}\cdot\vert (h_1, h_2)\vert^2
    \]
    so that $\omega(h_1, h_2) = o(h)$, and $D\omega(x_1, x_2)$ exists and is continuous, and is given by the \emph{linear map} $\omega(x_1, \cdot) + \omega(\cdot, x_2)$. The rest of the proof follows, if it is not immediately obvious then read the following note.
    \begin{note}
        Write $E = E_1\times E_2$ for convenience. The linear map $A = D\omega(x_1, x_2)$ takes arguments $E$ into $F$, consider the projections $\pi_1$ and $\pi_2$, and $v\in E_1\times E_2$, then
        \[A(v) = \omega(x_1, \pi_1 v) + \omega(\pi_2 v, x_2)\]
        We can view $A(x) = D\omega(x_1, x_2)\in L(E,F)$. It is clear that $A$ is linear in $x$, if we fix $v\in E$, 
        \[
            A(x+y, v) = \omega(\pi_1(x+y), \pi_2 v) + \omega(\pi_1 v, \pi_2 (x+y)) = A(x,v) + A(y,v)
        \]
        and similarly for scalar multiplication. Hence $DA(x) = A\in L(E, L(E,F))$ and $D^2A(x) = D^3\omega = 0$.
    \end{note}
\end{proof}

Our next result is the following, which states that if $f: U\to F$ where $U\osub E$, and $Df, DDf = D^2f$ exists and are continuous maps from $U$ into $L(E,F)$ and $L(E,L(E,F)$ respectively, then $D^2f(x)$ is a \emph{symmetric bilinear map}. The proof is non-trivial, and relies on computing the 'Lie Bracket':

\[
    D^2f(x)(v,w) - D^2f(x)(w,v)
\]

Which we will prove is equal to $0$ for every $x\in U$, and $v,w\in E$.

\begin{note}[Notation for open subsets]
The symbol '$\osub$' means $U$ is an open subset of $E$.    
\end{note}

\begin{wts}[Second derivative is symmetric]
    Let $f\in C^2(U, F)$, where $U\osub E$ with the possibility that $U = E$. For every point $x\in U$, the \emph{second derivative} $D^2f(x)$ is bilinear and symmetric.
\end{wts}
\begin{proof}
    Fix $x\in U\induces B(r) + x\osub U$. We restrict our attention to vectors $v,w\in E$ where $\vert v\vert, \vert w\vert < r2^{-1}$ for now, so that the
    \[
    \bigset{x, x+w, x+v, x+v+w}\subseteq U
    \]
    We will denote the following quantity by $\Delta$
    \[
        \Delta = f(x+w+v) - f(x+w) - f(x+v) + f(x)
    \]
    By rearranging terms, we see that $\Delta$ can be approximated in two ways:
    \begin{itemize}
        \item Postponing the discussion about the the domain of $y$, set $g(y) = f(y+v) - f(y)$ is $C^2$, and 
        \begin{equation}\label{second-derivative-Delta-g}
            \Delta = g(x+w) - g(x)
        \end{equation}
        \item Again, for $y$ sufficiently close to $x$, define $h(y) = f(y+w) - f(y)$, and
        \begin{equation}\label{second-derivative-Delta-h}
            \Delta = h(x+v) - h(x)
        \end{equation}
        \item To find the domain for $y$, an easy argument using the Triangle inequality gives us $g,h \in C^2(B(r2^{-1}) + x, F)$,
        \item Leaving the computations of $h$ as an exercise, we compute $Dg$, recall the shift map $y\mapsto y+v$ commutes with $D$, and
        \begin{equation}\label{second-derivative-Delta-Dg}
            Dg(y) = D(\tau_{-w}f)(y)  - Df(y) = Df(y+w) - Df(y)
        \end{equation}
    \end{itemize}

    Using MVT twice, once on \Cref{second-derivative-Delta-g} (the line segment $x + tw$, $0\leq t\leq 1$ is contained in the domain of $g$), and another time on \Cref{second-derivative-Delta-Dg} (with $y = x+tw$ in the integrand). We obtain:
    \begin{align*}
        \Delta &= g(x+w) - g(x)\\
        &= \int_{0}^1 Dg(x+tw)\cdot w dt\\
        &= \int_0^1\int_0^1 D^2f(x + tw+sv)\cdot v ds\: dt \cdot w\\
        &= \int_0^1\int_0^1 D^2f(x + tw + sv) ds\: dt \cdot v\cdot w
    \end{align*}

    % Justification for pulling out the duality pairing from the integral

    We can rewrite the application of $v$ then $w$ by $\cdot (v,w)$, and using the approximation $D^2f(x+tw+sv)\cdot(v,w) = D^2f(x)\cdot(v,w)+\delta_1(tw,sv)$. Integrating over $s,t$ gives

    \[
        \Delta = D^2f(x)\cdot(v,w) + \int_0^1\int_0^1 \delta_1(tw,sv) ds\: dt
    \]

    \begin{note}
        The error term $\delta_1$ in the integrand is given by
        \[
            \delta_1(tw,sv) = D^2f(x + tw + sv)(v,w) - D^2f(x)(v,w)
        \]
        for $v,w$ sufficiently small and $0 \leq s,t\leq 1$.
    \end{note}

    A similar argument for $h$ shows that $\Delta = D^2f(x)\cdot(w,v) + \int_0^1\int_0^1 \delta_2(tw,sv)ds\: dt$. Combining the two together, the following holds for all $v,w$ sufficiently small:

    \begin{equation}\label{second-derivative-lie-bracket}
        D^2f(x)\cdot (v,w) - D^2f(x)\cdot (w,v) = \int_0^1\int_0^1 \delta_1(tw,sv)ds\: dt - \int_0^1\int_0^1 \delta_2(tw,sv)ds\: dt
    \end{equation}

    To show the right hand side is $0$, we will need the following note. 

    \begin{note}
    We wish to show the RHS of \Cref{second-derivative-lie-bracket} is $0$. We begin by controlling the RHS and show that it is super-bilinear; meaning it shrinks after than the product $\abs{v}\abs{w}$. Then, we will prove a lemma which will show the only bilinear map that satisfies this property is the $0$ map.

    \begin{itemize}
        \item For $j=1,2$, relabel $\delta = \delta_j$ for convenience. We can use the $L^1$ inequality, to obtain the estimate
        \begin{equation}\label{second-derivative-step2}
            \abs{ \int_0^1\int_0^1\delta(tw,sv)ds\: dt} \leq \int_0^1\int_0^1\abs{\delta(tw,sv)}ds\: dt
        \end{equation}
        \item $\delta(tw,sv)$ is controlled by $\abs{D^2f(x + tw + sv) - D^2f(x)}\abs{v}\abs{w}$. Take $y = tw+sv$, then $\abs{y} \leq \abs{tw} + \abs{sv}$. Hence,
        \begin{equation}\label{second-derivative-control-on-rhs}
            \vert\delta_j\vert\leq\abs{D^2 f(x + tw + sv) - D^2f(x)}\abs{v}\abs{w}
        \end{equation}
        \item Let $A$ denote the span of $w,v$ for scalars $s,t\in[0,1]$. In symbols,
        \[
            A = \bigset{tw+sv,\: s,t\in [0,1]}
        \]

        $A$ is clearly compact, and the continuity of $D^2f$ means 
        
        \begin{equation}\label{second-derivative-R}
        R(v,w,\delta) = \sup_{y\in A}\abs{D^2f(x + y) - D^2f(x)}\quad\text{is finite},\qqtext{and}\lim_{(v,w)\to 0}R(v,w,\delta)=0
        \end{equation}
        
        See the remark after this proof for a generalized version of this 'compact linear combination' argument.
        
        \item Relabel $R(v,w)$ to be the maximum across $R(v,w,\delta_1)$ and $R(v,w,\delta_2)$. 
        \item Combining \Cref{second-derivative-step2,second-derivative-control-on-rhs,second-derivative-R}, we obtain the following bound on \Cref{second-derivative-lie-bracket}
        \begin{align}
            \abs{D^2f(x)\cdot(v,w) - D^2f(x)\cdot(w,v)} &\leq \abs{\iint\delta_1(tw,sv) ds\: dt - \iint\delta_2(tw,sv)ds\: dt}\nonumber\\
            &\leq \iint\abs{\delta_1}ds\: dt + \iint\abs{\delta_2}ds\: dt\nonumber \\
            &\leq \abs{v}\abs{w}R(v,w)\label{second-derivative-step3}
        \end{align}
    \end{itemize}

    The following Lemma gives a useful criterion to check when a multilinear map is identically $0$.
    \begin{lemma}
        Let $E$ be a Banach space, and $k\geq 1$ be an integer. If $\lambda\in L(E^k, F)$ and there exists another map $\theta: E^k\to F$ (defined perhaps on an open neighbourhood of the origin), such that

        \[
        \vert \lambda(\UL{u}[k])\vert \leq\vert\theta(\UL{u}[k])\vert\cdot\prod\vert \UL{u}[k]\vert
        \]
        
        for all $(\UL{u}[k])$ sufficiently small. And $\lim_{(\UL{u}[k]) \to 0}\theta(\UL{u}[k])=0$, then, $\lambda=0$.
    \end{lemma}
    \begin{proof}
        Fix arbitrary $(\UL{u}[k])\in E^k$, for $s>0$ sufficiently small, the left hand side of the equation reads

        \[
            \vert s\vert^k\vert\lambda (\UL{u}[k])\vert\leq\vert\theta(s\UL{u}[k])\vert\cdot\vert s\vert^k\prod \vert\UL{u}[k]\vert
        \]
        The rest of the argument is Archimedean: divide by $\vert s\vert^k$ and send $s\to 0$ (while paying attention to the term with $\theta$): perhaps after relabelling $v_s = s\UL{u}[k]$ for sufficiently small $s$, then $\vert \theta(v_s)\vert\to 0$ as $s\to 0$.
    \end{proof}
        
    \end{note}
    
    

\end{proof}
\begin{remark}
    Generalization of the "compact linear combination" argument used above. Let $(\UL{t}[k])\subseteq\mathbb{C}^k$ or $\real^k$, and vectors $\UL{v}[k]\in E$. Suppose further $(\UL{t}[k])\subseteq A$ is compact in $\mathbb{C}^k$ or $\realk$. It is clear that if $y = t_iv^i\in E$, where the summation convention is in effect. Then,
    \[
        \abs{y}\Lsim_A \abs{(v^{\underline{k}})}_{E^k}
    \]
    Now, fix a continuous function $f\in C(E,F)$, we can approximate the maximum error over all such $y$
    \[
        \sup_{y\in B}\abs{f(x+y) - f(x)}< \varepsilon\quad\forall \abs{y}\Lsim_A\vert(v^{\underline{k}})\vert < \delta
    \]

    where

    \[
    B = \bigset{\sum t_iv^i,\: (\UL{t}[k])\subseteq A,\: (v^{\underline{k}})\in E^k }
    \]
\end{remark}

\topheader{The $p$-th derivatives}
If $f$ is $p$ times differentiable, and $f, Df, D^2f, \ldots, D^pf$ are all continuous, then we say $f\in C^p(E,F)$ (replacing $E$ with an open subset of $E$ if necessary). A \emph{symmetric, $k$-linear} map between vector spaces $V,W$ is a map $A\in \mathcal{L}(V^k,W)$ such that for every $k$-permutation $\theta\in S_{\underline{k}}$, 
\[
    A(\Ul{v}[k]) = A(\Ul{v}[\theta(k)])    
\]
\begin{note}
    \begin{itemize}
        \item We say a map $F$ is \emph{between} the spaces $X$ and $Y$ if $F: X\to Y$. 
        \item $\mathcal{L}(V^K,W)$ denotes the space of $k$-linear maps from $V$ to $W$ that are not necessarily continuous. 
    \end{itemize}
\end{note}

\begin{wts}
    If $f\in C^p(E,F)$, then $D^pf(x)$ is symmetric for every $x\in E$. (Replace $E$ with an open set if necessary).
\end{wts}
\begin{proof}
    The main proof proceeds as follows. We will use induction on $p$, with $p=2$ serving as the base case. Our induction hypothesis is that for every $f\in C^{p-1}(E,F)$, for every permutation $\beta\in S_{p-1}$, at every point $x\in E$, for every possible choice of $p-1$ vectors $(v_2,\ldots, v_{p}) = (v_{1 + \underline{p-1}})$,
    \[
        D^{p-1}f(x)(v_{1+\underline{p-1}}) = D^{p-1}f(x)(v_{1+\beta(\underline{p-1})})
    \]
    To prove the assertion for $p$, it suffices to show $D^pf(x)(v_{\underline{p}})$ is invariant under transpositions of indices; since the transpositions generate $S_p$. Furthermore, the transpositions in $S_p$ are generated by 
    
    \begin{itemize}
        \item the transposition $(1,2,\ldots)\mapsto (2,1,\ldots)$ where the omitted indices are held fixed, and
        \item the transpositions which leave the first index fixed:
        \[
            (1,1+\underline{p-1})\mapsto (1,1+\beta({\underline{p-1}}))
        \]
        where $\beta\in S_{p-1}$
    \end{itemize}

    so it suffices to prove invariance under those two types of transpositions. Let $g = D^{p-2}f$, so $g\in C^2(E, L(E^{p-2}, F))$. Because the application of vectors (currying) on a multilinear map $A\in L(E^p, F)$ is associative, illustrated as follows:
    \[
       (A\cdot v_1)\cdot v_2 = A\cdot (v_1,v_2) = A(v_1, v_2,\cdot)\in L(E^{p-2}, F)
    \]
    Then, let $\lambda: L(E^{p-2}, F)\to F$ be the evaluation map at $(v_3,\ldots, v_p) = (v_{2+\underline{p-2}})$. 
    Using the base case on $D^{p-2}f = g\in C^2(E, L(E^{p-2}, F))$, 

    \[
        (D^2g)(x)(v_1,v_2) = (D^2g)(x)(v_2,v_1)\implies \lambda\qty\Big((D^2g)(x)(v_1,v_2)) = \lambda\qty\Big((D^2g)(x)(v_2,v_1))
    \]

    But $\lambda$ is the map that \emph{applies} the rest of the vectors, and
    \begin{equation}\label{second-derivative-D2g-equality}
        (D^2g)(x)(v_1,v_2)\cdot (v_{2+\underline{p-2}}) = (D^2g)(x)(v_2,v_1)\cdot (v_{2+\underline{p-2}})
    \end{equation}
    Since $D$ commutes with continuous linear maps (and $\lambda$ is continuous because $(v_{2+\underline{p-2}})$ is fixed),
    \begin{equation}\label{second-derivative-application-map-lambda-commute}
       \lambda(D^2(D^{p-2}f)) = D(\lambda(D(D^{p-2}f)) = D(D\lambda\circ D^{p-2}f) = D^2(\lambda\circ D^{p-2}f) 
    \end{equation}
    Substituting \Cref{second-derivative-D2g-equality} for the rightmost hand side of \Cref{second-derivative-application-map-lambda-commute} gives the result.
    \begin{note}
        There are no magic 'identifications' being made here. To be perfectly clear, for each $x\in E$, $g(x)$ is an element in $L(E^{p-2}, F)$, and $(D^2g)(x)\in L(E^2, L(E^{p-2}, F))$. Evaluating $g$ at a point $x$ gives a bilinear map that takes values in the Banach space $L(E^{p-2}, F)$.
    \end{note}

    For the second case, beginning from the induction hypothesis. If $\theta$ is a $p$-permutation that leaves the first coordinate unchanged, then there exists a unique $p-1$-permutation $\beta\in S_{p-1}$ such that
    \begin{align}\label{multi-derivatives-theta-def}
    \qty\big(\:\theta(\underline{p})\:) &= \qty\big(1,\theta(1+\underline{p-1}))\nonumber\\
    &= \qty\big(1,1+\beta(\underline{p-1}))    
    \end{align}
    Using a similar argument as the first case, set $g = D^{p-1}f$ and $\lambda, \lambda' \in L(E^{p-1}, F)$ to be the evaluation maps of $(v_1, v_{1+\underline{p-1}}) = (v_{\underline{p}})$ and $(v_1, v_{1+\beta(\underline{p-1})})$ respectively. Rehearsing the same proof as before:
    \begin{align*}
        (D^pf)(x)(v_{\underline{p}}) &= D\qty\big(\lambda D^{p-1}f)(x)(v_1)&&\text{\Cref{second-derivative-application-map-lambda-commute}}\\
        &= D\qty\big(\lambda' D^{p-1}f)(x)(v_1) &&\text{ind. hyp.}\\
        &= (D^pf)(x)(v_{\theta(\underline{p})})&&\text{\Cref{second-derivative-application-map-lambda-commute}}
    \end{align*}
    This proves the induction step, and the proof is complete.
    
\end{proof}
Before stating and proving Taylor's Theorem, an important remark on the 'postcomposition' of linear maps. Summarized in the following note. 

\begin{note}
    Let $f\in C^p(E,F)$, and $\lambda\in L^p(F,G)$. $\lambda$ induces a map between $L(E^p, F)$ and $L(E^p, G)$ by post-composing any multi-linear map $A\in L(E^p, F)$ by $\lambda$. Denoting this map by $\lambda_*$, 
\[
\lambda_*: L(E^p, F)\to L(E^p, G)
\]
It is clear $\lambda_*$ is linear and continuous. And its action on $A$, evaluated at $(v_{\underline{p}})\in E^p$ is given by
\[
    \lambda_*(A)\in L(E^p, G)\quad \qty\big(\lambda_*(A))(v_{\underline{p}}) = \lambda\qty\big(A(v_{\underline{p}})) = (\lambda\circ A)(v_{\underline{p}})
\]
Now, recall that for $p=1$
\[
    \qty\big[D(\lambda\circ f)](x) = \lambda\qty\big[(Df)(x)]
\]
To simplify the notation, we want to 'move' the evaluation $x$ outside of the brackets, and somehow write $x\mapsto \lambda\qty\big[(Df)(x)]$ as one map between $E$ and $L(E,G)$. We further \emph{identify} $\lambda$ as this map, so that
\[
    \qty\big[D(\lambda\circ f)](x) = \lambda  = \qty\big(\lambda \circ Df)(x)
\]
Dropping the $x$ from the expression, for $p\geq 2$ \emph{assuming a similar formula holds}, then we write $\qty\big[D^p(\lambda\circ f)] = \lambda_*\circ D^pf$. We make a final identification, of $\lambda = \lambda_*$ (thereby conflating the two different maps, the first is a map from $E$ to $F$, the second is a map from $L(E^p, F)$ into $L(E^p, G)$).
\end{note}

\begin{wts}
    If $p\geq 2$, $f\in C^p(E,F)$, $\lambda\in L(F,G)$, then
    \[
        D^p(\lambda\circ f) = \lambda\circ D^pf
    \]
    Where we have identified $\lambda$ as the same map that acts on $L(E^p, F)$ to produce another map in $L(E^p, G)$, and suppressed the point $x$.
\end{wts}
\begin{proof}
    Use induction on $p$.
\end{proof}


\newpage

\[
    \ln(e^{a}) = e^{\ln(a)} = a
\]
\end{document}

